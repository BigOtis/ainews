{"2023-05-07":[{"title":"Can ChatGPT Improve Racial Disparities In Healthcare?","link":"https://news.google.com/articles/CBMicGh0dHBzOi8vd3d3LmZvcmJlcy5jb20vc2l0ZXMvamFuaWNlZ2Fzc2FtLzIwMjMvMDUvMDYvaG93LWNoYXRncHQtY291bGQtaW1wcm92ZS1yYWNpYWwtZGlzcGFyaXRpZXMtaW4taGVhbHRoY2FyZS_SAXRodHRwczovL3d3dy5mb3JiZXMuY29tL3NpdGVzL2phbmljZWdhc3NhbS8yMDIzLzA1LzA2L2hvdy1jaGF0Z3B0LWNvdWxkLWltcHJvdmUtcmFjaWFsLWRpc3Bhcml0aWVzLWluLWhlYWx0aGNhcmUvYW1wLw?hl=en-US&gl=US&ceid=US%3Aen","image":"https://lh3.googleusercontent.com/proxy/a9YR7yf2q3xORobSw-EsP9CwCP20wf_t3i_tWkSWBJk7RXdjoUbUHcDm-T37w2tkfaBpUiOuwktLOvL3-Ng9gYGYdrbdmeVh5oi-uHtZCZ3iDzxZ_azZ50EW9AUg9wJ2zaQAf4UB-io0kzSlz9u4Vsc56Yj4rtWDjEZ5FPkdS5WnRfIDhZNeKyI5exFx7a6Wh9U5yC5lcWQjJCQ_EItC-4St9A=s0-w100-h100-rw-dcLTWMr5kJ","source":false,"datetime":"2023-05-06T19:26:12.000Z","time":"7 hours ago","related":[],"text":"\n    mirror-next-hop.forbes.com | Access denied (403)\n    Current session has been terminated.\n    For further information, do not hesitate to contact us.\n    Ref: 173.62.87.150 2023-05-07T03:09:18.231Z\n"},{"title":"ChatGPT's promising progress in healthcare","link":"https://news.google.com/articles/CBMiX2h0dHBzOi8vd3d3LmJlY2tlcnNob3NwaXRhbHJldmlldy5jb20vaW5ub3ZhdGlvbi9jaGF0Z3B0cy1wcm9taXNpbmctcHJvZ3Jlc3MtaW4taGVhbHRoY2FyZS5odG1s0gEA?hl=en-US&gl=US&ceid=US%3Aen","image":false,"source":false,"datetime":"2023-05-05T18:36:14.000Z","time":"Yesterday","related":[],"text":"ChatGPT, the AI-based chatbot developed by OpenAI, was released in November and has already been showing promising results for the healthcare industry as it can pass several benchmarking exams and was found to be more empathetic than physicians in answering patient questions. \n \nThe AI-based bot has been successful at passing four national benchmarking exams. \nChatGPT scored a 58 percent on a study exam used by physicians preparing for ophthalmology board certification, passed exams associated with the U.S. Medical Licensing Exam and passed a Stanford (Calif.) Medical School clinical reasoning exam with a score of 72 percent.\nIt was also found to be better at providing more empathetic answers to patient questions. \nIn an April 28 study published in JAMA Internal Medicine, researchers compared two sets of written responses to 195 real-world patient questions. \nThe physician responses were sourced from patient questions asked on Reddit, while the other responses were written by ChatGPT. \nThe sets of responses were evaluated by a three-member panel of licensed healthcare professionals, with 79 percent preferring the answers of ChatGPT. \nThe technology is also being piloted by health systems to see if it can tackle documentation burdens for clinicians.\nUC San Diego Health, Madison, Wis.-based UW Health and Palo Alto, Calif.-based Stanford Health Care are among the first healthcare organizations to pilot a new integration from Epic Systems and Microsoft that uses Azure OpenAI to draft messages within the EHR to patients. \nIf these pilots are successful, the new tool could have the potential to reduce administrative burden on physicians that costs U.S. healthcare $1 trillion annually.\nOther health systems and hospitals have also been gearing up for the new technology by hiring engineers who have expertise using ChatGPT.\nJohn Brownstein, PhD, chief innovation officer of Boston Children's, told Becker's that the organization is hiring a prompt engineer to work on large language models such as ChatGPT.\nDr. Brownstein said having an in-house expert on this technology will help Boston Children's identify use cases and train its workforce on the most appropriate uses.\nThe person will help Boston Children's design and develop prompts to effectively gather data from generative AI programs and refine the models for healthcare-specific applications.\nAlthough many organizations are in their early stages of studying and piloting these tools, many leaders are excited about the future with them, stating generative AI like ChatGPT can really bring change to the industry. \n\"We haven't seen this kind of level of innovation since the search engine or iPhone,\" Dr. Brownstein told Becker's. \"For instance, using AI to respond to patients' emails or to generate post-discharge notes automatically will help bring back a love of practicing medicine for many of our providers.\"Subscribe to the following topics: health itehremrelectronic health recordshealth information technologyLatest articles on Innovation:"},{"title":"The Top 6 Factors to Consider Before Using ChatGPT for Mental ...","link":"https://news.google.com/articles/CBMiTmh0dHBzOi8vd3d3Lm1ha2V1c2VvZi5jb20vZmFjdG9ycy1jb25zaWRlci1iZWZvcmUtdXNpbmctY2hhdGdwdC1tZW50YWwtaGVhbHRoL9IBAA?hl=en-US&gl=US&ceid=US%3Aen","image":"https://lh3.googleusercontent.com/proxy/A41iRzHEFhViV_tn0ngANEc7EVhQThElI3sqH12Yq9Ai4GCTqjM0FJIUVVeL_9hHAP7QhHt-UTUcA3IQ3ST9rDBzpr7xewlG1DVkZPx4FewfWEbhHiu6BSV24gX43Y-y7JFL27UkSNeaCStdJEV-iEsTS8GLvX_G6p_ivPEu=s0-w100-h100-rw-dcsaKUELUG","source":false,"datetime":"2023-05-06T13:45:00.000Z","time":"13 hours ago","related":[],"text":"\n \n\n Home Wellness  \n\n\n\n\n\n\n\nChatGPT can seem like a perfect resource for mental health information and advice, but there are some risks that should make you think twice.\n\n \n           \n\n\n  \nTools powered by artificial intelligence (AI) make daily life drastically easier. With large language models such as ChatGPT, access to information in a conversational way has improved. ChatGPT’s analytical features have found a large application in mental health. However, there are caveats to using it for mental health.\n\n\n  \nMental health conditions should only be diagnosed and treated by certified professionals. However, using AI to improve the management of symptoms has both advantages and disadvantages. While ChatGPT avoids giving medical advice, there are some factors to keep in mind before trusting it for mental health information.\n\n \n 1. ChatGPT Is Not a Replacement for Therapy \nChatGPT is a large language model trained on an enormous database of information. Therefore, it can generate human-like responses along with proper context. Such responses can help you learn about mental health but are not a replacement for in-person therapy.\n\nFor example, ChatGPT cannot diagnose an illness from your chats. However, it will give you an objective analysis but advise you to consult a medical professional. The analysis could be misinformed, so you must always fact-check it. Therefore, relying solely on AI for self-diagnosis could be detrimental to your mental well-being and should be avoided.\n\nUsing telehealth in place of in-person therapy is a better option. You can access mental health professionals remotely and at a significantly lower cost through telehealth services.\n\n\n\n 2. The Right Prompts Matter \n \n          \n\n\nThrough specific prompts, you can make better use of ChatGPT’s analytical ability and logical reasoning. It can act as a virtual companion rather than a therapist and provide various insights about mental health.\n\nThe more specific you are in your prompts, the better the responses. For example, a prompt like “List 10 ways to deal with social anxiety” provides a broad overview. It gives generic strategies rather than actionable advice for social situations.\n\nInstead, a more specific prompt will give you a significantly better response: “Provide me with practical tips and techniques to cope with social situations that trigger my anxiety. Include specific examples and situations that commonly cause social anxiety and any emotional and physical symptoms. Additionally, suggest resources or tools such as meditation, CBT, etc. to manage this issue and gradually build confidence.”\n\nYou can create great prompts by integrating your symptoms, some general questions about a condition, and a specific objective. That will help you use ChatGPT in a supportive and informative manner.\n\n\n\n 3. Spotting Misinformation \n \n          \n\n\nIt is crucial to spot misinformation while using ChatGPT. However, this can be challenging due to the confident tone used by the chatbot. Any kind of health claim requires peer-reviewed scientific evidence. Therefore, while using it for mental health, always ask it to cite studies that support any health claims.\n\nAnother error that ChatGPT is prone to making is presenting made-up information as facts. It sometimes responds with logically inconsistent or inaccurate information that can be harmful. For example, ChatGPT's limited training database cuts its access to updated scientific literature.\n\nAlso, it may produce wrong citations or links. Thus, manually checking claims using resources such as the PubMed search engine is essential. A great way to avoid incorrect responses is to limit your prompts to advice and analysis. While it can help you learn about various topics, refrain from using it to make conclusions and diagnose conditions.\n\n\n\n 4. Privacy Concerns With ChatGPT \n \n          \n\n\nAny kind of health information is personal. And ensuring that the health data collected by ChatGPT is not misused is not as easy. One of the main disadvantages is ChatGPT’s issues with privacy. OpenAI, the organization behind ChatGPT, states that your chat data is shared with service providers, affiliates, and other businesses.\n\nWhile your data may be anonymized (stripped of all personal identifiers), it is still subject to cybersecurity risks. Additionally, there is no confidentiality agreement for health-specific data. Therefore, OpenAI stores all your chat data on its servers for further use.\n\nThat may not be an issue if you do not enter personal information and sensitive health data. However, considering its overall impact on your data privacy, consulting a medical professional is much safer than using ChatGPT.\n\n\n\n 5. How ChatGPT Can Benefit Your Mental Health \nOne of the best ways to use the chatbot is for self-care, resource gathering, and education. Mental health can be a dense and vast subject to learn about. Whether you want to learn about a specific condition or overall best practices, information overload can affect your research.\n\nChatGPT helps you condense all that information into easily understandable points and illustrative examples. It can help you summarize books into insightful takeaway points which can help you determine if you want to read them.\n\nBy sorting through the clutter of information, you can reduce the stress that comes with research. The chatbot is also effective in providing self-care tips. It can help you with a practical plan to manage emotions such as anger, frustration, and sadness. Additionally, use it to create personalized routines and schedules that can help you organize the day better.\n\nYou can also delegate several productivity-associated tasks to it. This will free up some time and further help reduce stress levels throughout the day. Virtual assistants can also help you stay relaxed throughout the day.\n\n\n\n 6. Consider the Risks \n \n          \n\n\nWhile ChatGPT is a powerful tool for self-care and learning, it comes with some risks, including privacy, dependency, and bias in data. The dataset that the bot is trained on is human-generated, which is prone to several biases. Therefore, the type of response provided by ChatGPT may fluctuate based on these biases.\n\nDue to its instantaneous response times, personalized information has become extremely accessible. However, this also creates a risk of over-dependence on ChatGPT. The need to manually filter through search results and determine the best information is decreasing. In the long run, this may affect critical thinking, social interactions, and technological vulnerability.\n\n\n\n\nAs chatbot technology progresses, the responses will become more nuanced, logically sound, and informative. A newer version of the bot, GPT-4, has access to the internet and can extract more relevant data. However, some risks such as privacy concerns and bias remain. Therefore, using ChatGPT moderately and knowing how to spot health claims can help you avoid misinformation.\n\n "},{"title":"Could AI pen 'Casablanca'? Screenwriters take aim at ChatGPT ...","link":"https://news.google.com/articles/CBMiX2h0dHBzOi8vd3d3LnNlbnQtdHJpYi5jb20vMjAyMy8wNS8wNS9jb3VsZC1haS1wZW4tY2FzYWJsYW5jYS1zY3JlZW53cml0ZXJzLXRha2UtYWltLWF0LWNoYXRncHQv0gEA?hl=en-US&gl=US&ceid=US%3Aen","image":"https://lh3.googleusercontent.com/proxy/5jP8d8kKUcJNhn16SVFVbycd9BX1WiAZdRgh4uqGQOGFieaux9c46d7cdIic4H9qRWpqyZMrXCc5v_ws-ssfOr17hLGR2imHbZi4h2l6WgwC83yjoNNwgLbMtRZTLvGOTi7k7UmiVo3K-gt6eNAfyQs1I0BiA7_W29F0rdRstFUd6dtK815GYnjTM4G_=s0-w100-h100-rw-dcKRSE7ZUK","source":false,"datetime":"2023-05-05T16:30:00.000Z","time":"Yesterday","related":[],"text":"\nMembers of the The Writers Guild of America picket outside Fox Studios on Tuesday, May 2, 2023, in Los Angeles. Hollywood writers picketing to preserve pay and job security outside major studios and streamers braced for a long fight at the outset of a strike that immediately forced late-night shows into hiatus and numerous other productions on hold. (AP Photo/Ashley Landis)\nNEW YORK (AP) — When Greg Brockman, the president and co-founder of ChatGPT maker OpenAI, was recently extolling the capabilities of artificial intelligence, he turned to “Game of Thrones.” \nImagine, he said, if you could use AI to rewrite the ending of that not-so-popular finale. Maybe even put yourself into the show.\n“That is what entertainment will look like,” said Brockman.\nNot six months since the release of ChatGPT, generative artificial intelligence is already prompting widespread unease throughout Hollywood. Concern over chatbots writing or rewriting scripts is one of the leading reasons TV and film screenwriters took to picket lines earlier this week.\nThough the Writers Guild of America is striking for better pay in an industry where streaming has upended many of the old rules, AI looms as rising anxiety.\n“AI is terrifying,” said Danny Strong, the “Dopesick” and “Empire” creator. “Now, I’ve seen some of ChatGPT’s writing and as of now I’m not terrified because Chat is a terrible writer. But who knows? That could change.”\nAI chatbots, screenwriters say, could potentially be used to spit out a rough first draft with a few simple prompts (“a heist movie set in Beijing”). Writers would then be hired, at a lower pay rate, to punch it up.\nScreenplays could also be slyly generated in the style of known writers. What about a comedy in the voice of Nora Ephron? Or a gangster film that sounds like Mario Puzo? You won’t get anything close to “Casablanca” but the barest bones of a bad Liam Neeson thriller isn’t out of the question.\nThe WGA’s basic agreement defines a writer as a “person” and only a human’s work can be copyrighted. But even though no one’s about to see a “By AI” writers credit at the beginning a movie, there are myriad ways that regenerative AI could be used to craft outlines, fill in scenes and mock up drafts.\n“We’re not totally against AI,” says Michael Winship, president of the WGA East and a news and documentary writer. “There are ways it can be useful. But too many people are using it against us and using it to create mediocrity. They’re also in violation of copyright. They’re also plagiarizing.”\nThe guild is seeking more safeguards on how AI can be applied to screenwriting. It says the studios are stonewalling on the issue. The Alliance of Motion Picture and Television Producers, which bargains on the behalf of production companies, has offered to annually meet with the guild to go over definitions around the fast-evolving technology.\n“It’s something that requires a lot more discussion, which we’ve committed to doing,” the AMPTP said in an outline of its position released Thursday.\nExperts say the struggle screenwriters are now facing with regenerative AI is just the beginning. The World Economic Forum this week released a report predicting that nearly a quarter of all jobs will be disrupted by AI over the next five years.\n“It’s definitely a bellwether in the workers’ response to the potential impacts of artificial intelligence on their work,” says Sarah Myers West, managing director of the nonprofit AI Now Institute, which has lobbied the government to enact more regulation around AI. “It’s not lost on me that a lot of the most meaningful efforts in tech accountability have been a product of worker-led organizing.”\nAI has already filtered into nearly every part of moviemaking. It’s been used to de-age actors, remove swear words from scenes in post-production, supply viewing recommendations on Netflix and posthumously bring back the voices of Anthony Bourdain and Andy Warhol.\nThe Screen Actors Guild, set to begin its own bargaining with the AMPTP this summer, has said it’s closely following the evolving legal landscape around AI.\n“Human creators are the foundation of the creative industries and we must ensure that they are respected and paid for their work,” the actors union said.\nThe implications for screenwriting are only just being explored. Actors Alan Alda and Mike Farrell recently reconvened to read through a new scene from “M(asterisk)A(asterisk)S(asterisk)H” written by ChatGPT. The results weren’t terrible, though they weren’t so funny, either.\n“Why have a robot write a script and try to interpret human feelings when we already have studio executives who can do that?” deadpanned Alda.\nWriters have long been among notoriously exploited talents in Hollywood. The films they write usually don’t get made. If they do, they’re often rewritten many times over. Raymond Chandler once wrote “the very nicest thing Hollywood can possibly think to say to a writer is that he is too good to be only a writer.”\nScreenwriters are accustomed to being replaced. Now, they see a new, readily available and inexpensive competitor in AI — albeit one with a slightly less tenuous grasp of the human condition.\n“Obviously, AI can’t do what writers and humans can do. But I don’t know that they believe that, necessarily,” says screenwriter Jonterri Gadson (“A Black Lady Sketchshow”). “There needs to be a human writer in charge and we’re not trying to be gig workers, just revising what AI does. We need to tell the stories.”\nDramatizing their plight as man vs. machine surely doesn’t hurt the WGA’s cause in public opinion. The writers are wrestling with the threat of AI just as concern widens over how hurriedly regenerative AI products has been thrust into society.\nGeoffrey Hinton, an AI pioneer, recently left Google in order to speak freely about its potential dangers. “It’s hard to see how you can prevent the bad actors from using it for bad things,” Hinton told The New York Times.\n“What’s especially scary about it is nobody, including a lot of the people who are involved with creating it, seem to be able to explain exactly what it’s capable of and how quickly it will be capable of more,” says actor-screenwriter Clark Gregg.\nThe writers finds themselves in the awkward position of negotiating on a newborn technology with the potential for radical effect. Meanwhile, AI-crafted songs by “Fake Drake” or “Fake Eminem” continue to circulate online.\n“They’re afraid that if the use of AI to do all this becomes normalized, then it becomes very hard to stop the train,” says James Grimmelmann, a professor of digital and information law at Cornell University. “The guild is in the position of trying to imagine lots of different possible futures.”\nIn that way, the long work stoppage that many are expecting — Moody’s Investor Service forecasts that the strike may last three months or longer — could offer more time to analyze how regenerative AI might reshape screenwriting.\nRegister to recieve the FREE Sentinel Tribune daily newsletter!Get all of the day's top stories delivered right to your inbox every morning.In the meantime, chanting demonstrators are hoisting signs with messages aimed at a digital foe. Seen on the picket lines: “ChatGPT doesn’t have childhood trauma”; “I heard AI refuses to take notes”; and “Wrote ChatGPT this.” ___ Associated Press Writer Krysta Fauria in Los Angeles and Robert Bumsted and Aron Ranen in New York contributed to this report.\n___ Follow AP Film Writer Jake Coyle on Twitter at: http://twitter.com/jakecoyleAP\n "},{"title":"These ChatGPT Rivals Are Designed to Play With Your Emotions","link":"https://news.google.com/articles/CBMiQWh0dHBzOi8vd3d3LndpcmVkLmNvbS9zdG9yeS9mYXN0LWZvcndhcmQtY2hhdGdwdC1yaXZhbHMtZW1vdGlvbnMv0gEA?hl=en-US&gl=US&ceid=US%3Aen","image":"https://lh3.googleusercontent.com/proxy/ojHLNuXPRIcz81FeLdUnTvIKcUbu80Bj8cjW1zu8ZUmc-0Sqwgd9s-j_sDgUyIw_s026bz73kHpZWvv9uao-V2813Zhu_eNQ39BGJOMMq_GCQDd8d8cholu5Fsx3pWBsq0ixG6jbwfzdfcqFVy_Y6ofef87DaXfwe1MRnXaHIgCfnSC_6rIiMKPpMqGwLoVNRcyZXfvI2qCI=s0-w100-h100-rw-dcKWSMbikG","source":false,"datetime":"2023-05-04T16:00:00.000Z","time":"2 days ago","related":[],"text":"ChatGPT and its brethren are both surprisingly clever and disappointingly dumb. Sure, they can generate pretty poems, solve scientific puzzles, and debug spaghetti code. But we know that they often fabricate, forget, and act like weirdos.Inflection AI, a company founded by researchers who previously worked on major artificial intelligence projects at Google, OpenAI, and Nvidia, built a bot called Pi that seems to make fewer blunders and be more adept at sociable conversation.Inflection designed Pi to address some of the problems of today’s chatbots. Programs like ChatGPT use artificial neural networks that try to predict which words should follow a chunk of text, such as an answer to a user’s question. With enough training on billions of lines of text written by humans, backed by high-powered computers, these models are able to come up with coherent and relevant responses that feel like a real conversation. But they also make stuff up and go off the rails. Mustafa Suleyman, Inflection’s CEO, says the company has carefully curated Pi’s training data to reduce the chance of toxic language creeping into its responses. “We're quite selective about what goes into the model,” he says. “We do take a lot of information that’s available on the open web, but not absolutely everything.”Suleyman, who cofounded the AI company Deepmind, which is now part of Google, also says that limiting the length of Pi’s replies reduces—but does not wholly eliminate—the likelihood of factual errors.Based on my own time chatting with Pi, the result is engaging, if more limited and less useful than ChatGPT and Bard. Those chatbots became better at answering questions through additional training in which humans assessed the quality of their  responses. That feedback is used to steer the bots toward more satisfying responses. Suleyman says Pi was trained in a similar way, but with an emphasis on being friendly and supportive—though without  a human-like personality, which could confuse users about the program’s capabilities. Chatbots that take on a human persona have already proven problematic. Last year, a Google engineer controversially claimed that the company’s AI model LaMDA, one of the first programs to demonstrate how clever and engaging large AI language models could be, might be sentient. Pi is also able to keep a record of all its conversations with a user, giving it a kind of long-term memory that is missing in ChatGPT and is intended to add consistency to its chats.“Good conversation is about being responsive to what a person says, asking clarifying questions, being curious, being patient,” says Suleyman. “It’s there to help you think, rather than give you strong directional advice, to help you to unpack your thoughts.”Pi adopts a chatty, caring persona, even if it doesn’t pretend to be human. It often asked how I was doing and frequently offered words of encouragement. Pi’s short responses mean it would also work well as a voice assistant, where long-winded answers and errors are especially jarring. You can try talking with it yourself at Inflection's website.The incredible hype around ChatGPT and similar tools means that many entrepreneurs are hoping to strike it rich in the field. Suleyman used to be a manager within the Google team working on the LaMDA chatbot. Google was hesitant to release the technology, to the frustration of some of those working on it who believed it had big commercial potential."},{"title":"OpenAI contractors make $15 to train ChatGPT","link":"https://news.google.com/articles/CBMicWh0dHBzOi8vd3d3Lm5iY25ld3MuY29tL3RlY2gvaW5ub3ZhdGlvbi9vcGVuYWktY2hhdGdwdC1haS1qb2JzLWNvbnRyYWN0b3JzLXRhbGstc2hhZG93LXdvcmtmb3JjZS1wb3dlcnMtcmNuYTgxODky0gEqaHR0cHM6Ly93d3cubmJjbmV3cy5jb20vbmV3cy9hbXAvcmNuYTgxODky?hl=en-US&gl=US&ceid=US%3Aen","image":"https://lh3.googleusercontent.com/proxy/gNIzHI_EWU3pj8GaBSDO67IJHjU6Zisyhs6ekeFK5bbr2US_zkavZFUUg2jCDEjqmiWfmMNeadUdCv6055dX2y2qXVeSDww6oNt-ZtG7PLmsS8IMhhEinFQWSAthiyLd8HWh9ZFGWfMSIO_l8FJN5NTqapI3-hV7AXs0YEfDY3HIzzyqaUlK=s0-w100-h100-rw-dcnSOI6z0F","source":false,"datetime":"2023-05-06T15:00:00.000Z","time":"12 hours ago","related":[],"text":"Alexej Savreux, a 34-year-old in Kansas City, says he’s done all kinds of work over the years. He’s made fast-food sandwiches. He’s been a custodian and a junk-hauler. And he’s done technical sound work for live theater. These days, though, his work is less hands-on: He’s an artificial intelligence trainer. Savreux is part of a hidden army of contract workers who have been doing the behind-the-scenes labor of teaching AI systems how to analyze data so they can generate the kinds of text and images that have wowed the people using newly popular products like ChatGPT. To improve the accuracy of AI, he has labeled photos and made predictions about what text the apps should generate next. The pay: $15 an hour and up, with no benefits. Out of the limelight, Savreux and other contractors have spent countless hours in the past few years teaching OpenAI’s systems to give better responses in ChatGPT. Their feedback fills an urgent and endless need for the company and its AI competitors: providing streams of sentences, labels and other information that serve as training data. “We are grunt workers, but there would be no AI language systems without it,” said Savreux, who’s done work for tech startups including OpenAI, the San Francisco company that released ChatGPT in November and set off a wave of hype around generative AI. “You can design all the neural networks you want, you can get all the researchers involved you want, but without labelers, you have no ChatGPT. You have nothing,” Savreux said. It’s not a job that will give Savreux fame or riches, but it’s an essential and often overlooked one in the field of AI, where the seeming magic of a new technological frontier can overshadow the labor of contract workers. “A lot of the discourse around AI is very congratulatory,” said Sonam Jindal, the program lead for AI, labor and the economy at the Partnership on AI, a nonprofit based in San Francisco that promotes research and education around artificial intelligence. “But we’re missing a big part of the story: that this is still hugely reliant on a large human workforce,” she said. The tech industry has for decades relied on the labor of thousands of lower-skilled, lower-paid workers to build its computer empires: from punch-card operators in the 1950s to more recent Google contractors who’ve complained about second-class status, including yellow badges that set them apart from full-time employees. Online gig work through sites like Amazon Mechanical Turk grew even more popular early in the pandemic. Now, the burgeoning AI industry is following a similar playbook. The work is defined by its unsteady, on-demand nature, with people employed by written contracts either directly by a company or through a third-party vendor that specializes in temp work or outsourcing. Benefits such as health insurance are rare or nonexistent — which translates to lower costs for tech companies — and the work is usually anonymous, with all the credit going to tech startup executives and researchers. The Partnership on AI warned in a 2021 report that a spike in demand was coming for what it called “data enrichment work.” It recommended that the industry commit to fair compensation and other improved practices, and last year it published voluntary guidelines for companies to follow. DeepMind, an AI subsidiary of Google, is so far the only tech company to publicly commit to those guidelines. “A lot of people have recognized that this is important to do. The challenge now is to get companies to do it,” Jindal said. “This is a new job that’s being created by AI,” she added. “We have the potential for this to be a high-quality job and for workers who are doing this work to be respected and valued for their contributions to enabling this advancement.” A spike in demand has arrived, and some AI contract workers are asking for more. In Nairobi, Kenya, more than 150 people who’ve worked on AI for Facebook, TikTok and ChatGPT voted Monday to form a union, citing low pay and the mental toll of the work, Time magazine reported. Facebook and TikTok did not immediately respond to requests for comment on the vote. OpenAI declined to comment. So far, AI contract work hasn’t inspired a similar movement in the U.S. among the Americans quietly building AI systems word-by-word.Savreux, who works from home on a laptop, got into AI contracting after seeing an online job posting. He credits the AI gig work — along with a previous job at the sandwich chain Jimmy John’s — with helping to pull him out of homelessness. “People sometimes minimize these necessary, laborious jobs,” he said. “It’s the necessary, entry-level area of machine learning.” The $15 an hour is more than the minimum wage in Kansas City. Job postings for AI contractors refer to both the allure of working in a cutting-edge industry as well as the sometimes-grinding nature of the work. An advertisement from Invisible Technologies, a temp agency, for an “Advanced AI Data Trainer” notes that the job would be entry level with pay starting at $15 an hour, but also that it could be “beneficial to humanity.” “Think of it like being a language arts teacher or a personal tutor for some of the world’s most influential technology,” the job posting says. It doesn’t name Invisible’s client, but it says the new hire would work “within protocols developed by the world’s leading AI researchers.” Invisible did not immediately respond to a request for more information on its listings. There’s no definitive tally of how many contractors work for AI companies, but it’s an increasingly common form of work around the world. Time magazine reported in January that OpenAI relied on low-wage Kenyan laborers to label text that included hate speech or sexually abusive language so that its apps could do better at recognizing toxic content on their own. OpenAI has hired about 1,000 remote contractors in places such as Eastern Europe and Latin America to label data or train company software on computer engineering tasks, the online news outlet Semafor reported in January. OpenAI is still a small company, with some 375 employees as of January, CEO Sam Altman said on Twitter, but that number doesn’t include contractors and doesn’t reflect the full scale of the operation or its ambitions. A spokesperson for OpenAI said no one was available to answer questions about its use of AI contractors. The work of creating data to train AI models isn’t always simple to do, and sometimes it’s complex enough to attract would-be AI entrepreneurs. Jatin Kumar, a 22-year-old in Austin, Texas, said he’s been doing AI work on contract for a year since he graduated college with a degree in computer science, and he said it gives him a sneak peak into where generative AI technology is headed in the near-term. “What it allows you to do is start thinking about ways to use this technology before it hits public markets,” Kumar said. He’s also working on his own tech startup, Bonsai, which is making software to help with hospital billing. A conversational trainer, Kumar said his main work has been generating prompts: participating in a back-and-forth conversation with chatbot technology that’s part of the long process of training AI systems. The tasks have grown more complex with experience, he said, but they started off very simple. “Every 45 or 30 minutes, you’d get a new task, generating new prompts,” he said. The prompts might be as simple as, “What is the capital of France?” he said. Kumar said he worked with about 100 other contractors on tasks to generate training data, correct answers and fine-tune the model by giving feedback on answers. He said other workers handled “flagged” conversations: reading over examples submitted by ChatGPT users who, for one reason or another, reported the chatbot’s answer back to the company for review. When a flagged conversation comes in, he said, it’s sorted based on the type of error involved and then used in further training of the AI models. “Initially, it started off as a way for me to help out at OpenAI and learn about existing technologies,” Kumar said. “But now, I can’t see myself stepping away from this role.” "}]}